{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYQyUGtxAiI3"
   },
   "source": [
    "# Shakespeare Text Generation (using RNN LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CELdk7T_BDcT"
   },
   "source": [
    "In this study, we aim to generate text that resembles Shakespeare's writing style through the use of a Recurrent Neural Network (RNN) that operates on a character-based level. The source material for this experiment is drawn from the Shakespeare dataset featured in the blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). The implementation will be done using Tensorflow v2 and its Keras API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_x-znHvB8S8"
   },
   "source": [
    "_Inspired by [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation)_\n",
    "and [Kaggle_Notebook](https://www.kaggle.com/code/aashkatrivedi/shakespeare-text-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfK2qRcwCIh2"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code imports TensorFlow, Matplotlib, and NumPy libraries and prints the Python version, TensorFlow version, and Keras version installed in the current environment. It also prints the platform (operating system) and current time, but those statements are commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WlwA0iiOCLFM",
    "outputId": "4f938569-6d5a-4a32-bc11-3343c683e9e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.13\n",
      "Tensorflow version: 2.11.0\n",
      "Keras version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import platform\n",
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "print('Python version:', platform.python_version())\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('Keras version:', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5njVZzbVCsrM"
   },
   "source": [
    "## Downloading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code downloads a text file containing the complete works of William Shakespeare from a URL specified in dataset_file_origin. The file is downloaded to a directory specified in cache_dir, and the path to the downloaded file is printed. The get_file function from the TensorFlow Keras module is used to download the file and ensure that it is only downloaded once. The file is downloaded with the name shakespeare.txt and its absolute path is determined using the pathlib library.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zbs-beR_Cwu4",
    "outputId": "9df5f15c-c40a-4115-8b16-7284168be695"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n",
      "C:\\Users\\varun\\Downloads\\tmp\\datasets\\shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "cache_dir = './tmp'\n",
    "dataset_file_name = 'shakespeare.txt'\n",
    "dataset_file_origin = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "\n",
    "dataset_file_path = tf.keras.utils.get_file(\n",
    "    fname=dataset_file_name,\n",
    "    origin=dataset_file_origin,\n",
    "    cache_dir=pathlib.Path(cache_dir).absolute()\n",
    ")\n",
    "\n",
    "print(dataset_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRHGHegWC27k"
   },
   "source": [
    "## Analyzing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code reads the Shakespeare text file that was previously downloaded and prints the number of characters in the text using the len function. The text file is opened in read mode using the built-in open function and its contents are stored in the text variable. The length of text is then printed using the format method to insert the value of len(text) into the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDZJG573C6v2",
    "outputId": "74174e92-6641-4eca-e3cb-157282d6d1e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Reading the database file.\n",
    "text = open(dataset_file_path, mode='r').read()\n",
    "\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Exploration- This code prints the first 300 characters in the Shakespeare text file stored in the text variable using array slicing. The [:300] notation indicates that the first 300 characters of text should be returned.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vkkO4KIPC9WC",
    "outputId": "5b98adad-10de-4203-ce59-6fd12504f49b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the first 300 characters in text.\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Exploration - This code finds the unique characters in the Shakespeare text file and prints the number of unique characters and the list of unique characters sorted in alphabetical order. The set function is used to create a set of unique characters from the text variable. The sorted function is then used to sort the set into a list in alphabetical order and assign it to the vocab variable. The number of unique characters is printed using the format method to insert the length of vocab into the string. The vocab list is also printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrsJvLYjDGYU",
    "outputId": "0308a8cc-8a98-4558-af3f-e4d07e2a2959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n",
      "vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "print('vocab:', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBo1d4dXDbjF"
   },
   "source": [
    "# Processing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rD9TzFACDhTb"
   },
   "source": [
    "##Text Vectorization\n",
    "\n",
    "Before feeding the text to our RNN we need to convert the text from a sequence of characters to a sequence of numbers. To do so we will detect all unique characters in the text, form a vocabulary out of it and replace each character with its index in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Exploration - This code creates a dictionary char2index that maps each character in the vocab list to its index in the list. The dictionary is created using a dictionary comprehension, where the keys are the characters and the values are the indices. The code then prints the first 20 key-value pairs of the char2index dictionary using a loop and the zip function. The loop iterates over the first 20 keys in the dictionary and prints each key-value pair using the format method. The repr function is used to obtain a string representation of the character that is safe to use in Python code, even if it contains special characters like quotes. Finally, the code prints an ellipsis to indicate that there are more key-value pairs in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54xOh3zSDqd_",
    "outputId": "f2d36929-79f1-4e11-cac9-49ac4b576ffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '$' :   3,\n",
      "  '&' :   4,\n",
      "  \"'\" :   5,\n",
      "  ',' :   6,\n",
      "  '-' :   7,\n",
      "  '.' :   8,\n",
      "  '3' :   9,\n",
      "  ':' :  10,\n",
      "  ';' :  11,\n",
      "  '?' :  12,\n",
      "  'A' :  13,\n",
      "  'B' :  14,\n",
      "  'C' :  15,\n",
      "  'D' :  16,\n",
      "  'E' :  17,\n",
      "  'F' :  18,\n",
      "  'G' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Map characters to their indices in vocabulary.\n",
    "char2index = {char: index for index, char in enumerate(vocab)}\n",
    "\n",
    "print('{')\n",
    "for char, _ in zip(char2index, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2index[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Exploration - This code creates an array index2char that maps the indices of characters in the vocab list to the characters themselves. The np.array function from NumPy is used to create the array, and the vocab list is passed as the argument. The resulting array has the same length as the vocab list, and the value at each index of the array is the character corresponding to that index in the vocab list. The array is printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4932gVhlEUwE",
    "outputId": "967b29bc-e8d0-4587-9a5a-44360463e027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "# Map character indices to characters from vacabulary.\n",
    "index2char = np.array(vocab)\n",
    "print(index2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Exploration- This code converts the characters in the text variable to their corresponding indices in the char2index dictionary and stores the resulting indices in a NumPy array text_as_int. The conversion is done using a list comprehension, where the char2index dictionary is used to look up the index of each character in the text variable. The resulting list is passed to the np.array function to create a NumPy array. The length of the text_as_int array is printed using the format method to insert the length of the array into the string. The first 15 characters of the original text and their corresponding indices in the text_as_int array are also printed using the format method and the repr function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ht5wIvV3EVOZ",
    "outputId": "c691a6d1-ad64-414d-fc00-b2d620556a36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_as_int length: 1115394\n",
      "'First Citizen:\\n' --> array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0])\n"
     ]
    }
   ],
   "source": [
    "# Convert chars in text to indices.\n",
    "text_as_int = np.array([char2index[char] for char in text])\n",
    "\n",
    "print('text_as_int length: {}'.format(len(text_as_int)))\n",
    "print('{} --> {}'.format(repr(text[:15]), repr(text_as_int[:15])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zkuTLe1EdUo"
   },
   "source": [
    "# Create Training Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets the maximum length of a sentence in characters to sequence_length and calculates the number of examples that will be generated in a single epoch of training data. The number of examples is calculated by dividing the length of the text variable by sequence_length + 1. The + 1 is added to account for the label that is shifted by one character from the input sequence. The result is printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCWTzN8kEhXo",
    "outputId": "f3e3eebe-a153-4102-ec02-6bcdbdd29a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_per_epoch: 11043\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters.\n",
    "sequence_length = 100\n",
    "examples_per_epoch = len(text) // (sequence_length + 1)\n",
    "\n",
    "print('examples_per_epoch:', examples_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a tf.data.Dataset object char_dataset from the text_as_int NumPy array. The from_tensor_slices method is used to create a dataset where each element is a single character index from the text_as_int array. The code then iterates over the first 5 elements of the dataset and prints the corresponding characters using the index2char array to look up the characters. The take method is used to extract the first 5 elements of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdSxZfTxFCeI",
    "outputId": "64d7b992-ef5e-4954-ac35-c3d8b8c2ebd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset.\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for char in char_dataset.take(5):\n",
    "    print(index2char[char.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a new dataset sequences where each element is a sequence of characters with length sequence_length + 1 (accounting for the label). The batch method is used to create the sequences, with the drop_remainder=True parameter indicating that any remaining elements that don't fit evenly into a sequence should be dropped. The length of the sequences dataset is printed using the as_numpy_iterator method, which converts the dataset to a NumPy array and returns an iterator over the elements. The first 5 sequences in the dataset are printed using a for loop that iterates over the sequences dataset and prints each sequence using the index2char array to look up the corresponding characters. The join method is used to concatenate the characters in each sequence into a single string. The repr function is used to ensure that any special characters (like line breaks) are printed as escape sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1JbOiZRFF67",
    "outputId": "3dcb17ad-52d6-4696-d4d0-e70ac0fb0af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences count: 11043\n",
      "\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "# Generate batched sequences out of the char_dataset.\n",
    "sequences = char_dataset.batch(sequence_length + 1, drop_remainder=True)\n",
    "\n",
    "# Sequences size is the same as examples_per_epoch.\n",
    "print('Sequences count: {}'.format(len(list(sequences.as_numpy_iterator()))));\n",
    "print()\n",
    "\n",
    "# Sequences examples.\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(index2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comment describes the shape of the sequences dataset. The dataset contains 11,043 sequences, where each sequence has a length of 101 characters (100 characters for the input and 1 character for the label). The comment provides a visual representation of the shape of the dataset as a list of tuples, where each tuple contains a sequence of 101 characters. The entire dataset is represented as a list of these tuples, with a length of 11,043."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kyFMVfa6FNh9"
   },
   "outputs": [],
   "source": [
    "# sequences shape:\n",
    "# - 11043 sequences\n",
    "# - Each sequence of length 101\n",
    "#\n",
    "#\n",
    "#    101     101          101\n",
    "# [(.....) (.....) ...  (.....)]\n",
    "#\n",
    "# <---------- 11043 ----------->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3_kBQN8FZsL"
   },
   "source": [
    "For each sequence, duplicate and shift it to form the input and target text. For example, say `sequence_length` is `4` and our text is `Hello`. The input sequence would be `Hell`, and the target sequence `ello`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Exploration- This function takes a sequence of indices as input and returns a tuple (input_text, target_text) where input_text is the sequence of indices with the last element removed, and target_text is the same sequence of indices with the first element removed. In other words, input_text is the sequence of indices that will be fed as input to the model, and target_text is the sequence of indices that the model is trying to predict. The function can be used to convert the sequences in the sequences dataset into input-target pairs that can be used to train the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ck4hUPE3FaZq"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Exploratrion - This code applies the split_input_target function to each element of the sequences dataset using the map method, to create a new dataset dataset where each element is a tuple of two sequences: the input sequence and the target sequence. The size of the dataset is the same as the sequences dataset, which is equal to examples_per_epoch. The input and target sequences have a length of sequence_length, which is one less than the length of the sequences in the sequences dataset. The as_numpy_iterator() method is used to iterate over the dataset and convert it to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pnp1DR5qGwA7",
    "outputId": "4b06f357-08ec-4c45-f076-f59a6ec1d518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 11043\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# Dataset size is the same as examples_per_epoch.\n",
    "# But each element of a sequence is now has length of `sequence_length`\n",
    "# and not `sequence_length + 1`.\n",
    "print('dataset size: {}'.format(len(list(dataset.as_numpy_iterator()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Exploration - This code selects the first example from the dataset, using the take(1) method, and prints the size of the input and target sequences. It also prints the input and target sequences, using the join method to convert the sequences of indices to sequences of characters.\n",
    "\n",
    "The input sequence has length sequence_length, which is the same as the length of the input_example sequence, and the target sequence has the same length. The input sequence is the first 100 characters of the text, and the target sequence is the same as the input sequence, but shifted one character to the right. The goal of the language model will be to predict the next character in the target sequence, given the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWhje97BHBMj",
    "outputId": "ede3af04-864a-4627-a29c-93967f5baa2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence size: 100\n",
      "Target sequence size: 100\n",
      "\n",
      "Input: 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input sequence size:', repr(len(input_example.numpy())))\n",
    "    print('Target sequence size:', repr(len(target_example.numpy())))\n",
    "    print()\n",
    "    print('Input:', repr(''.join(index2char[input_example.numpy()])))\n",
    "    print('Target:', repr(''.join(index2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comment provides a description of the shape of the dataset object.\n",
    "\n",
    "The dataset object has examples_per_epoch sequences, each of length sequence_length. Each element of the dataset object is a tuple of two sub-sequences of length sequence_length: input_text and target_text.\n",
    "\n",
    "input_text contains the first 100 characters of each sequence, and target_text contains the last 100 characters of each sequence.\n",
    "\n",
    "This format allows us to train a language model that takes as input a sequence of length sequence_length, and outputs a probability distribution over the vocabulary for the next character in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_zQaC-EyHEnu"
   },
   "outputs": [],
   "source": [
    "# dataset shape:\n",
    "# - 11043 sequences\n",
    "# - Each sequence is a tuple of 2 sub-sequences of length 100 (input_text and target_text)\n",
    "#\n",
    "#\n",
    "#    100       100           100\n",
    "# /(.....)\\ /(.....)\\ ... /(.....)\\  <-- input_text\n",
    "# \\(.....)/ \\(.....)/     \\(.....)/  <-- target_text\n",
    "#\n",
    "# <----------- 11043 ------------->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqCG3w8dHL6a"
   },
   "source": [
    "Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the RNN considers the previous step context in addition to the current input character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Exploration - This code is just showing the first 5 steps of the first sequence in the dataset. For each step, it prints the input index, which corresponds to a character in the input sequence, and the expected output index, which corresponds to the next character in the target sequence. It also prints the actual character corresponding to each index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9w5VBoKHMr1",
    "outputId": "755df565-a17a-4aa8-e50c-50d27719ed7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0\n",
      "  input: 18 ('F')\n",
      "  expected output: 47 ('i')\n",
      "Step  1\n",
      "  input: 47 ('i')\n",
      "  expected output: 56 ('r')\n",
      "Step  2\n",
      "  input: 56 ('r')\n",
      "  expected output: 57 ('s')\n",
      "Step  3\n",
      "  input: 57 ('s')\n",
      "  expected output: 58 ('t')\n",
      "Step  4\n",
      "  input: 58 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print('Step {:2d}'.format(i))\n",
    "    print('  input: {} ({:s})'.format(input_idx, repr(index2char[input_idx])))\n",
    "    print('  expected output: {} ({:s})'.format(target_idx, repr(index2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSpTCKQvTq8H"
   },
   "source": [
    "## Splitting training sequences into batches\n",
    "\n",
    "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a TensorFlow dataset that is shuffled and batched, using the parameters BUFFER_SIZE and BATCH_SIZE to control the size of the shuffle buffer and the size of the batches. The drop_remainder argument is set to True, which means that any incomplete batches at the end of the dataset will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OhGe4Dc-TtPT",
    "outputId": "6d9719e9-dd87-4149-ed3b-578b26d7679a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int32, name=None), TensorSpec(shape=(64, 100), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset (TF data is designed to work\n",
    "# with possibly infinite sequences, so it doesn't attempt to shuffle\n",
    "# the entire sequence in memory. Instead, it maintains a buffer in\n",
    "# which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input_text and target_text tensors contain sequences of characters, where each character is represented by an integer. The first batch of input sequences is a 64x100 tensor, where 64 is the batch size and 100 is the sequence length. The target_text tensor is identical to the input_text tensor except that each element is shifted one index to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyGLbynGTv8p",
    "outputId": "5521f5df-3ea2-4b15-94cb-5e13e5a66fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st batch: input_text: tf.Tensor(\n",
      "[[53 52 43 ... 41 47 58]\n",
      " [47 57  1 ... 45 57  1]\n",
      " [ 6  1 63 ... 41 43  1]\n",
      " ...\n",
      " [ 0 32 46 ... 43 57 58]\n",
      " [52 41 43 ... 53  1 19]\n",
      " [57 41 43 ... 52 42  1]], shape=(64, 100), dtype=int32)\n",
      "\n",
      "1st batch: target_text: tf.Tensor(\n",
      "[[52 43 10 ... 47 58 47]\n",
      " [57  1 45 ... 57  1 57]\n",
      " [ 1 63 53 ... 43  1 47]\n",
      " ...\n",
      " [32 46 53 ... 57 58  6]\n",
      " [41 43  6 ...  1 19 53]\n",
      " [41 43 54 ... 42  1 50]], shape=(64, 100), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for input_text, target_text in dataset.take(1):\n",
    "    print('1st batch: input_text:', input_text)\n",
    "    print()\n",
    "    print('1st batch: target_text:', target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is processing text data to train a character-level language model using TensorFlow.\n",
    "\n",
    "It begins by loading a text corpus and creating a vocabulary of characters used in the corpus. The characters in the corpus are then converted to numerical indices that correspond to their positions in the vocabulary. Next, the text is divided into sequences of fixed length (100 in this case), and these sequences are used to create a training dataset. Each sequence is split into two parts: the first part is used as the input to the model, and the second part is used as the target that the model should predict. Finally, the data is batched and shuffled to create a training dataset that can be used to train the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZEXe-Rj8UEyP"
   },
   "outputs": [],
   "source": [
    "# dataset shape:\n",
    "# - 172 batches\n",
    "# - 64 sequences per batch\n",
    "# - Each sequence is a tuple of 2 sub-sequences of length 100 (input_text and target_text)\n",
    "#\n",
    "#\n",
    "#     100       100           100             100       100           100\n",
    "# |/(.....)\\ /(.....)\\ ... /(.....)\\| ... |/(.....)\\ /(.....)\\ ... /(.....)\\|  <-- input_text\n",
    "# |\\(.....)/ \\(.....)/     \\(.....)/| ... |\\(.....)/ \\(.....)/     \\(.....)/|  <-- target_text\n",
    "#\n",
    "# <------------- 64 ---------------->     <------------- 64 ---------------->\n",
    "#\n",
    "# <--------------------------------- 172 ----------------------------------->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewGm_WKCUNP3"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "Use [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) to define the model. For this simple example three layers are used to define our model:\n",
    "\n",
    "- [tf.keras.layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding): The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
    "- [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): A type of RNN with size units=rnn_units (You can also use a GRU layer here.)\n",
    "- [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense): The output layer, with vocab_size outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szuJkrYcUQhq",
    "outputId": "34b1ab0f-7643-49f4-fda0-1059ddcc0283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n",
      "tmp_input_array shape: (2, 8)\n",
      "tmp_input_array:\n",
      "[[8 0 4 0 1 5 3 2]\n",
      " [1 9 8 4 8 3 2 2]]\n",
      "\n",
      "tmp_output_array shape: (2, 8, 5)\n",
      "tmp_output_array:\n",
      "[[[ 0.02616686  0.03458718  0.04411212  0.00989461  0.01564747]\n",
      "  [ 0.00553139 -0.04481349  0.04246651  0.00104336 -0.02476038]\n",
      "  [-0.03420416  0.0273389   0.00014647  0.0045799  -0.02062111]\n",
      "  [ 0.00553139 -0.04481349  0.04246651  0.00104336 -0.02476038]\n",
      "  [ 0.01248703 -0.00867982  0.00561149  0.03468699 -0.00750459]\n",
      "  [ 0.02547472 -0.03056592 -0.03166564 -0.00810779 -0.04666475]\n",
      "  [ 0.00074784  0.02251151 -0.04139213 -0.02150704 -0.01593578]\n",
      "  [ 0.0319855   0.01490427 -0.04178299 -0.03205196 -0.04446663]]\n",
      "\n",
      " [[ 0.01248703 -0.00867982  0.00561149  0.03468699 -0.00750459]\n",
      "  [-0.01804072  0.00212531 -0.04117199 -0.02517644  0.04818705]\n",
      "  [ 0.02616686  0.03458718  0.04411212  0.00989461  0.01564747]\n",
      "  [-0.03420416  0.0273389   0.00014647  0.0045799  -0.02062111]\n",
      "  [ 0.02616686  0.03458718  0.04411212  0.00989461  0.01564747]\n",
      "  [ 0.00074784  0.02251151 -0.04139213 -0.02150704 -0.01593578]\n",
      "  [ 0.0319855   0.01490427 -0.04178299 -0.03205196 -0.04446663]\n",
      "  [ 0.0319855   0.01490427 -0.04178299 -0.03205196 -0.04446663]]]\n"
     ]
    }
   ],
   "source": [
    "# Let's do a quick detour and see how Embeding layer works.\n",
    "# It takes several char indices sequences (batch) as an input.\n",
    "# It encodes every character of every sequence to a vector of tmp_embeding_size length.\n",
    "tmp_vocab_size = 10\n",
    "tmp_embeding_size = 5\n",
    "tmp_input_length = 8\n",
    "tmp_batch_size = 2\n",
    "\n",
    "tmp_model = tf.keras.models.Sequential()\n",
    "tmp_model.add(tf.keras.layers.Embedding(\n",
    "  input_dim=tmp_vocab_size,\n",
    "  output_dim=tmp_embeding_size,\n",
    "  input_length=tmp_input_length\n",
    "))\n",
    "# The model will take as input an integer matrix of size (batch, input_length).\n",
    "# The largest integer (i.e. word index) in the input should be no larger than 9 (tmp_vocab_size).\n",
    "# Now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "tmp_input_array = np.random.randint(\n",
    "  low=0,\n",
    "  high=tmp_vocab_size,\n",
    "  size=(tmp_batch_size, tmp_input_length)\n",
    ")\n",
    "tmp_model.compile('rmsprop', 'mse')\n",
    "tmp_output_array = tmp_model.predict(tmp_input_array)\n",
    "\n",
    "print('tmp_input_array shape:', tmp_input_array.shape)\n",
    "print('tmp_input_array:')\n",
    "print(tmp_input_array)\n",
    "print()\n",
    "print('tmp_output_array shape:', tmp_output_array.shape)\n",
    "print('tmp_output_array:')\n",
    "print(tmp_output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Exploration - We will use a simple architecture consisting of an Embedding layer, followed by a LSTM layer, and a Dense layer with vocab_size output units to predict the probability distribution over the next character in the sequence. The Embedding layer maps each character to a vector of embedding_dim dimensions, which is learned during training. The LSTM layer processes the sequence of input embeddings, and the output of the LSTM is passed through the Dense layer to produce the probability distribution over the vocabulary.\n",
    "\n",
    "We will train the model to minimize the cross-entropy loss between the predicted and true probability distributions over the vocabulary. During training, we will use teacher forcing, where the input to the LSTM at each time step is the true target character at the previous time step, rather than the predicted character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "VbMJ3nEFUUDb"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars.\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension.\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units.\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function builds and returns a Keras model with three layers:\n",
    "\n",
    "1. An Embedding layer that takes integer-encoded vocabulary indices and turns them into dense vectors of fixed size.\n",
    "2. An LSTM layer with rnn_units number of units. This layer processes the input sequences and produces output sequences of the same length.\n",
    "3. A Dense layer that maps the output sequences of the LSTM layer to a probability distribution over the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "rkjkh6fDUWu7"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "      input_dim=vocab_size,\n",
    "      output_dim=embedding_dim,\n",
    "      batch_input_shape=[batch_size, None]\n",
    "    ))\n",
    "\n",
    "    model.add(tf.keras.layers.LSTM(\n",
    "      units=rnn_units,\n",
    "      return_sequences=True,\n",
    "      stateful=True,\n",
    "      recurrent_initializer=tf.keras.initializers.GlorotNormal()\n",
    "    ))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(vocab_size))\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now defined with the architecture we specified using the build_model function, and can be further compiled and trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "O7bGUsTqUZLb"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbhscVFVUcOR",
    "outputId": "64d5ce59-7592-45d9-b4d5-7ad8ffebd7bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IGdymosVtI2"
   },
   "source": [
    "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahUc_k7qUuI6"
   },
   "source": [
    "## Try the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shape of the model on a batch of input examples is (batch_size, sequence_length, vocab_size). This means that for each sequence in the batch, the model outputs a vector of probabilities for each character in the vocabulary. The batch_size and sequence_length dimensions correspond to the input sequences in the batch. The vocab_size dimension corresponds to the probability distribution over the vocabulary for each input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJ7ptjsCUuqc",
    "outputId": "b16521a3-8e05-4af1-a80e-4c1a17ce6130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMNW7ZmjU1or"
   },
   "source": [
    "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example_batch_predictions tensor contains predictions for each step of the sequence, for each sequence in the batch. The shape of the tensor is (batch_size, sequence_length, vocab_size).\n",
    "\n",
    "So, example_batch_predictions[0, 0] is the prediction of the model for the first character of the first sequence in the batch. It is a vector of size vocab_size, representing the probability distribution over the vocabulary for this character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UxqR1gqyU2FI",
    "outputId": "10e1993a-16cb-4bb6-fec9-302047ba8edf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the 1st letter of the batch 1st sequense:\n",
      "tf.Tensor(\n",
      "[ 1.2368873e-03  4.9343780e-03 -5.8752312e-03 -4.7560653e-04\n",
      "  3.0634673e-03  4.3385196e-05 -2.7023735e-03 -1.9164273e-03\n",
      " -3.7364969e-03 -3.7354586e-04  1.7429364e-03 -2.5310314e-03\n",
      " -3.6916151e-03 -5.4574152e-03 -3.5941799e-03 -3.3951791e-03\n",
      "  2.6757554e-03 -4.7201272e-03 -3.1726656e-03  3.0335097e-03\n",
      " -5.4603415e-03 -4.1199426e-04 -2.5574279e-03 -2.5612814e-03\n",
      "  1.4487673e-03  4.5793541e-04 -9.4073848e-04  2.3000021e-03\n",
      "  5.4469756e-03  4.0158904e-03  1.1975446e-03  2.0692854e-03\n",
      "  6.5231426e-03 -2.6291418e-03  4.5868242e-03 -4.0359520e-03\n",
      " -7.0128404e-03 -1.5679910e-03  7.0456997e-03 -3.5346572e-03\n",
      "  3.3441803e-03  4.4654096e-03  2.0483318e-03  2.1035396e-03\n",
      " -5.0911899e-03 -1.0357515e-03  2.5092871e-03 -2.3523911e-03\n",
      " -5.1094475e-03 -6.4994809e-03  3.0526691e-03 -7.4863178e-04\n",
      "  1.9131664e-03 -5.1045017e-03 -3.7387223e-03 -7.7903247e-04\n",
      "  1.6829667e-03  2.5114915e-03 -6.7370210e-04  1.4942887e-03\n",
      " -1.9098228e-03 -1.8726361e-03 -5.4713921e-04  1.4524832e-03\n",
      "  5.2531473e-03], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('Prediction for the 1st letter of the batch 1st sequense:')\n",
    "print(example_batch_predictions[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.random.categorical() function generates random samples from a categorical distribution. It takes in a logits tensor which contains unnormalized log probabilities for each class, and returns a tensor of shape [batch_size, num_samples] where each element is an integer corresponding to the index of the chosen class. In the example shown, tmp_samples is a 1D tensor of length 5, with each element representing a random sample from the categorical distribution defined by the logits [-0.95, 0, 0.95]. The distribution is such that the class with index 2 has the highest probability of being chosen, followed by class 1 and then class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXWJtoTKU3k5",
    "outputId": "48985a80-63d2-4cbf-bb77-aee1f192060f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1 2 2 1 1]], shape=(1, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Quick overview of how tf.random.categorical() works.\n",
    "\n",
    "# logits is 2-D Tensor with shape [batch_size, num_classes].\n",
    "# Each slice [i, :] represents the unnormalized log-probabilities for all classes.\n",
    "# In the example below we say that the probability for class \"0\" is low but the\n",
    "# probability for class \"2\" is much higher.\n",
    "tmp_logits = [\n",
    "  [-0.95, 0, 0.95],\n",
    "];\n",
    "\n",
    "# Let's generate 5 samples. Each sample is a class index. Class probabilities \n",
    "# are being taken into account (we expect to see more samples of class \"2\").\n",
    "tmp_samples = tf.random.categorical(\n",
    "    logits=tmp_logits,\n",
    "    num_samples=5\n",
    ")\n",
    "\n",
    "print(tmp_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of sampled_indices is (100, 1). This means that num_samples is 1, so sampled_indices contains a single sample for each of the 100 time steps in the sequence. The values of sampled_indices are integers between 0 and the size of the vocabulary minus 1, which represent the predicted character indices for each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hEaQ639AU6hj",
    "outputId": "d40afb78-09fc-4f44-cecc-b02c36f57982"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(\n",
    "    logits=example_batch_predictions[0],\n",
    "    num_samples=1\n",
    ")\n",
    "\n",
    "sampled_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "squeeze() function removes dimensions of size 1 from the shape of a tensor. Here, we removed the dimension of size 1 from sampled_indices which was introduced by the tf.random.categorical() function. We then convert the resulting tensor to a numpy array to inspect its shape. Since sampled_indices is a 2D tensor, its shape should be (batch_size, sequence_length) where batch_size is the number of sequences in a batch, and sequence_length is the length of each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5H97hygPU83o",
    "outputId": "35a8f161-b701-48cd-9e54-3300eb46c546"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.squeeze(\n",
    "    input=sampled_indices,\n",
    "    axis=-1\n",
    ").numpy()\n",
    "\n",
    "sampled_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an array of integer values that represent the predicted index of the next character in the sequence, as sampled from the output probability distribution of the model. Each integer value corresponds to a character in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCF6Lax3U-Ti",
    "outputId": "9bc6d742-2ed4-43be-9698-b92f99114223"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48, 12, 22, 33,  4, 32, 45,  0, 28, 46, 10, 58, 15, 55, 55,  4,  2,\n",
       "       61,  8, 14, 34, 12, 22, 47, 20, 28, 61, 55, 51, 12,  0, 49, 23, 41,\n",
       "       19, 11, 22,  4, 39, 40, 35, 20,  2, 28,  8,  7, 16, 22, 43, 28, 13,\n",
       "       19, 49, 11, 27, 42,  3, 21, 12, 48, 59, 13, 53,  1,  5, 24, 36,  0,\n",
       "       39, 32, 56, 26, 12, 30,  5, 38, 30,  7, 21, 42, 58, 35,  3,  4, 22,\n",
       "       31, 21, 35,  1,  8, 57, 37, 47, 40, 61, 60, 15, 11, 34,  9])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above displays the input sequence and the predicted next character for that sequence, obtained by sampling from the model's output probability distribution.\n",
    "\n",
    "For the input sequence, it concatenates the characters by joining their corresponding string representations from the index2char mapping.\n",
    "\n",
    "The predicted next character is obtained by selecting the character index with the highest probability from the predicted probability distribution output by the model. The character is then converted back to its string representation using the index2char mapping, and all the predicted characters are concatenated to form the predicted output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1xAwiqSVAb6",
    "outputId": "f1c83187-a537-4451-c300-e2d0e5c36a6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " 'o confess\\nI loved him as in honour he required,\\nWith such a kind of love as might become\\nA lady like'\n",
      "\n",
      "Next char prediction:\n",
      " \"j?JU&Tg\\nPh:tCqq&!w.BV?JiHPwqm?\\nkKcG;J&abWH!P.-DJePAGk;Od$I?juAo 'LX\\naTrN?R'ZR-IdtW$&JSIW .sYibwvC;V3\"\n"
     ]
    }
   ],
   "source": [
    "print('Input:\\n', repr(''.join(index2char[input_example_batch[0]])))\n",
    "print()\n",
    "print('Next char prediction:\\n', repr(''.join(index2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates the loss for a batch of predictions generated by the model. The loss function is defined as sparse_categorical_crossentropy with from_logits=True, which means that it expects the model predictions to be in the form of unnormalized log probabilities (logits). The example_batch_loss tensor contains the calculated loss values for each sequence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGpZgFDOVCqC",
    "outputId": "0a34cb3f-8268-460a-9fac-75a5a661e038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction  0\n",
      "  input: 53 ('o')\n",
      "  next predicted: 1 ('j')\n",
      "Prediction  1\n",
      "  input: 1 (' ')\n",
      "  next predicted: 1 ('?')\n",
      "Prediction  2\n",
      "  input: 41 ('c')\n",
      "  next predicted: 1 ('J')\n",
      "Prediction  3\n",
      "  input: 53 ('o')\n",
      "  next predicted: 1 ('U')\n",
      "Prediction  4\n",
      "  input: 52 ('n')\n",
      "  next predicted: 1 ('&')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, sample_idx) in enumerate(zip(input_example_batch[0][:5], sampled_indices[:5])):\n",
    "    print('Prediction {:2d}'.format(i))\n",
    "    print('  input: {} ({:s})'.format(input_idx, repr(index2char[input_idx])))\n",
    "    print('  next predicted: {} ({:s})'.format(target_idx, repr(index2char[sample_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIs0ZdXpVSJc"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvkLg0SiV4lC"
   },
   "source": [
    "### Attach an optimizer, and a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines an objective function loss using sparse_categorical_crossentropy from the tf.keras.losses module. This function computes the cross-entropy loss between the true labels labels and the predicted logits probabilities.\n",
    "\n",
    "The from_logits argument is set to True which indicates that the logits values are the output of a dense layer without applying softmax activation. This is because sparse_categorical_crossentropy internally applies softmax activation on the logits before calculating the loss, which leads to improved numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzuAruPVV5Cx",
    "outputId": "53ded4e8-3c09-4413-ce90-719ce424ac21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.1743093\n"
     ]
    }
   ],
   "source": [
    "# An objective function.\n",
    "# The function is any callable with the signature scalar_loss = fn(y_true, y_pred).\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "      y_true=labels,\n",
    "      y_pred=logits,\n",
    "      from_logits=True\n",
    "    )\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code compiles the model with the specified optimizer and loss function. Here, Adam optimizer is used with a learning rate of 0.001 and the loss function is defined as sparse_categorical_crossentropy. The model is now ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "etg6ar3qV7TJ"
   },
   "outputs": [],
   "source": [
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=adam_optimizer,\n",
    "    loss=loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXSNXT1RWA5T"
   },
   "source": [
    "### Configure checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code sets up a ModelCheckpoint callback that will save the model weights after each epoch. The filepath argument specifies the path and filename format for the saved weights, including the string {epoch} to insert the current epoch number. The save_weights_only argument indicates that only the model weights, not the entire model, should be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "s1QajDvPV9g6"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved.\n",
    "checkpoint_dir = 'tmp/checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ha2ddQ9ZWF2K"
   },
   "source": [
    "### Execute the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Exploration - The EPOCHS variable is used to specify the number of epochs (iterations over the entire dataset) to train the model. In this case, the model will be trained for 100 epochs. The Source code was trained on 60 epochs. We tried to train the model on more epochs as training a neural network for too few epochs can lead to underfitting, whereas training for too many epochs can lead to overfitting. The optimal number of epochs will depend on the complexity of the problem, the size of the dataset, and the architecture of the model. Increasing the number of epochs can lead to better performance up to a certain point, but beyond that point, the model may start to overfit to the training data, resulting in poor performance on new, unseen data. Therefore, it is important to monitor the performance of the model on a validation set during training and stop training when the validation loss stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "lGoyXlF7WDEJ"
   },
   "outputs": [],
   "source": [
    "EPOCHS=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model.fit() function trains the model on the dataset for the given number of epochs using the specified optimizer and loss function. In this case, it uses the Adam optimizer and sparse categorical cross-entropy loss. The training is done in batches of size 64, which is specified in the BATCH_SIZE variable defined earlier.\n",
    "\n",
    "The checkpoint_callback is passed as a callback to save the model weights after each epoch. These weights can be used later to restore the trained model.\n",
    "\n",
    "The training progress and loss are logged in the history object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e76x9BGmWJLQ",
    "outputId": "63a45c7f-c980-49c1-b3d3-e3e85e2e349c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "172/172 [==============================] - 18s 74ms/step - loss: 2.6167\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 14s 68ms/step - loss: 1.9050\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 13s 69ms/step - loss: 1.6483\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 1.5088\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 1.4251\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 13s 68ms/step - loss: 1.3668\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 1.3227\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 15s 71ms/step - loss: 1.2829\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 1.2459\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 1.2106\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 1.1739\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 1.1379\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 1.0982\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 1.0597\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 1.0188\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.9756\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 14s 72ms/step - loss: 0.9335\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.8927\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.8528\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 15s 69ms/step - loss: 0.8144\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 13s 69ms/step - loss: 0.7773\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.7425\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.7112\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.6824\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.6548\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.6315\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.6090\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.5909\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.5732\n",
      "Epoch 30/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.5592\n",
      "Epoch 31/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.5434\n",
      "Epoch 32/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.5292\n",
      "Epoch 33/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.5187\n",
      "Epoch 34/100\n",
      "172/172 [==============================] - 15s 69ms/step - loss: 0.5079\n",
      "Epoch 35/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.4981\n",
      "Epoch 36/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.4887\n",
      "Epoch 37/100\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.4810\n",
      "Epoch 38/100\n",
      "172/172 [==============================] - 14s 68ms/step - loss: 0.4751\n",
      "Epoch 39/100\n",
      "172/172 [==============================] - 13s 68ms/step - loss: 0.4684\n",
      "Epoch 40/100\n",
      "172/172 [==============================] - 14s 67ms/step - loss: 0.4622\n",
      "Epoch 41/100\n",
      "172/172 [==============================] - 14s 67ms/step - loss: 0.4572\n",
      "Epoch 42/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.4531\n",
      "Epoch 43/100\n",
      "172/172 [==============================] - 13s 68ms/step - loss: 0.4462\n",
      "Epoch 44/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.4435\n",
      "Epoch 45/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.4414\n",
      "Epoch 46/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.4365\n",
      "Epoch 47/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.4328\n",
      "Epoch 48/100\n",
      "172/172 [==============================] - 14s 67ms/step - loss: 0.4305\n",
      "Epoch 49/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.4291\n",
      "Epoch 50/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.4251\n",
      "Epoch 51/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.4236\n",
      "Epoch 52/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.4192\n",
      "Epoch 53/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.4156\n",
      "Epoch 54/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.4167\n",
      "Epoch 55/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.4148\n",
      "Epoch 56/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.4124\n",
      "Epoch 57/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.4097\n",
      "Epoch 58/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.4079\n",
      "Epoch 59/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.4076\n",
      "Epoch 60/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.4058\n",
      "Epoch 61/100\n",
      "172/172 [==============================] - 14s 66ms/step - loss: 0.4056\n",
      "Epoch 62/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.4037\n",
      "Epoch 63/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.4020\n",
      "Epoch 64/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.4016\n",
      "Epoch 65/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.3998\n",
      "Epoch 66/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.3988\n",
      "Epoch 67/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.3999\n",
      "Epoch 68/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.3988\n",
      "Epoch 69/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.3967\n",
      "Epoch 70/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.3975\n",
      "Epoch 71/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.3974\n",
      "Epoch 72/100\n",
      "172/172 [==============================] - 15s 67ms/step - loss: 0.3967\n",
      "Epoch 73/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.3938\n",
      "Epoch 74/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3937\n",
      "Epoch 75/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3934\n",
      "Epoch 76/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.3925\n",
      "Epoch 77/100\n",
      "172/172 [==============================] - 14s 66ms/step - loss: 0.3904\n",
      "Epoch 78/100\n",
      "172/172 [==============================] - 14s 67ms/step - loss: 0.3911\n",
      "Epoch 79/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3911\n",
      "Epoch 80/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3900\n",
      "Epoch 81/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3906\n",
      "Epoch 82/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3897\n",
      "Epoch 83/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3913\n",
      "Epoch 84/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3899\n",
      "Epoch 85/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3883\n",
      "Epoch 86/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3905\n",
      "Epoch 87/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.3888\n",
      "Epoch 88/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3878\n",
      "Epoch 89/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3877\n",
      "Epoch 90/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3873\n",
      "Epoch 91/100\n",
      "172/172 [==============================] - 14s 66ms/step - loss: 0.3849\n",
      "Epoch 92/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3806\n",
      "Epoch 93/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3812\n",
      "Epoch 94/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3815\n",
      "Epoch 95/100\n",
      "172/172 [==============================] - 13s 66ms/step - loss: 0.3807\n",
      "Epoch 96/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.3835\n",
      "Epoch 97/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.3862\n",
      "Epoch 98/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.3885\n",
      "Epoch 99/100\n",
      "172/172 [==============================] - 13s 67ms/step - loss: 0.3897\n",
      "Epoch 100/100\n",
      "172/172 [==============================] - 14s 67ms/step - loss: 0.3914\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  x=dataset,\n",
    "  epochs=EPOCHS,\n",
    "  callbacks=[\n",
    "    checkpoint_callback\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Exploaration - This function takes the training history of a model (i.e., the output of the fit method) and plots the training loss across epochs using matplotlib. The plot has the following characteristics:\n",
    "\n",
    "1. The x-axis represents the epochs.\n",
    "2. The y-axis represents the loss value.\n",
    "3. The plot title is \"Loss\".\n",
    "4. The plot has a label for the training set.\n",
    "5. The plot has a legend.\n",
    "6. The plot has a grid with a dashed line style, a width of 1, and an opacity of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "aQt4ZfNWWLVK"
   },
   "outputs": [],
   "source": [
    "def render_training_history(training_history):\n",
    "    loss = training_history.history['loss']\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(loss, label='Training set')\n",
    "    plt.legend()\n",
    "    plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Exploaration - This function takes in the history object returned by model.fit() and generates a plot of the training loss over time (epochs). The function accesses the training loss values from the history object and plots them on a graph with labeled axes and a legend. Finally, it displays the plot.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "IagKIEnKKhUk",
    "outputId": "4cf215cc-7d01-48b8-82a7-3c0711ba4279"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "render_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maogxNhhVhgB"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfkuhUtsVhtv"
   },
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xCnkEA8VkUh"
   },
   "source": [
    "### Restore the latest checkpoint\n",
    "\n",
    "To keep this prediction step simple, use a batch size of 1.\n",
    "\n",
    "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
    "\n",
    "To run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.train.latest_checkpoint() function returns the path of the latest checkpoint file in a directory. In this case, it will return the path of the latest saved checkpoint in the checkpoint_dir directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "uJOnQtu6KjJT",
    "outputId": "883183bb-9fc7-46aa-b9b7-6fcf46cb4d75"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'tmp/checkpoints/ckpt_100'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is loading the weights of the trained model from the latest checkpoint saved in the checkpoint_dir directory, and then building a new model with a batch_size of 1, so that we can generate text one character at a time. It's important to note that the weights of the trained model can only be loaded into a new model with the same architecture. Therefore, we first need to create a new model with the same architecture as the one that was trained, and then we can load the weights into it.\n",
    "\n",
    "Also, the new model is built with a batch_size of 1 because when generating text, we want to provide the model with a single character at a time, and get the next predicted character. The model is then built with a dynamic sequence length, which means that it can handle input sequences of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "7gsGfZ5WVqEU"
   },
   "outputs": [],
   "source": [
    "simplified_batch_size = 1\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([simplified_batch_size, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0mG4NRtQuSYi",
    "outputId": "411157bb-8dc0-43a6-85b4-e22d13446219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (1, None, 256)            16640     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (1, None, 1024)           5246976   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 65)             66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIKuUu55uq28"
   },
   "source": [
    "### The prediction loop\n",
    "\n",
    "The following code block generates the text:\n",
    "\n",
    "- It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
    "\n",
    "- Get the prediction distribution of the next character using the start string and the RNN state.\n",
    "\n",
    "- Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
    "\n",
    "- The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates text of length num_generate using the provided model and a starting string start_string. The function first converts the start_string to a sequence of input indices, initializes an empty string text_generated, and resets the state of the model. The model then predicts the next character using the input sequence and the previous hidden state.\n",
    "\n",
    "The temperature parameter controls the degree of randomness in the output text. A lower temperature produces more predictable text, while a higher temperature produces more surprising text. The function uses the tf.random.categorical function to sample the predicted character based on the predicted probabilities, using a categorical distribution. The generated character is then added to the text_generated string, and the process is repeated until num_generate characters have been generated. Finally, the function returns the concatenation of the start_string and the generated text_generated string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "bcFMFn4NukKz"
   },
   "outputs": [],
   "source": [
    "# num_generate\n",
    "# - number of characters to generate.\n",
    "#\n",
    "# temperature\n",
    "# - Low temperatures results in more predictable text.\n",
    "# - Higher temperatures results in more surprising text.\n",
    "# - Experiment to find the best setting.\n",
    "def generate_text(model, start_string, num_generate = 1000, temperature=1.0):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing).\n",
    "    input_indices = [char2index[s] for s in start_string]\n",
    "    input_indices = tf.expand_dims(input_indices, 0)\n",
    "\n",
    "    # Empty string to store our results.\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1.\n",
    "    model.reset_states()\n",
    "    for char_index in range(num_generate):\n",
    "        predictions = model(input_indices)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Using a categorical distribution to predict the character returned by the model.\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(\n",
    "        predictions,\n",
    "        num_samples=1\n",
    "        )[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state.\n",
    "        input_indices = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(index2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code generates text starting with the string \"ROMEO: \" using the trained model. It uses the generate_text() function and passes the model and the starting string to it as arguments. The default temperature is set to 1.0, which controls the \"creativity\" of the generated text. A higher temperature results in more unpredictable text, while a lower temperature results in more predictable text. The function generates 1000 characters of text and returns the generated string, which is then printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zu7G5VMPuuBX",
    "outputId": "26d7e5e4-4e5a-4390-95c3-e720f1d4e396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: so perhaps he doth:\n",
      "Alter, at the outward sighs;\n",
      "My scrudle princess, help, heapsing war\n",
      "In so most ignorance.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "To hear Phroches of this blood,\n",
      "Nor never nursed in and like your foul reason\n",
      "For raising EDWICK:\n",
      "From off the gods know him and to make him answer\n",
      "The noble duke hath been thishthy death,\n",
      "But lusty, young wrinkled me.\n",
      "\n",
      "ANGELO:\n",
      "When I word to prove his head. Pray you, sit now at the least\n",
      "Of Hermione a spirit true that I saw her and not vex'd.\n",
      "With this, for we'll she kill to enter thee.\n",
      "\n",
      "KING RICHARD II:\n",
      "Thou hast sail tell me, is it possible\n",
      "That lone! the king's King formal shuniefellows in a power; but I am still\n",
      "As fearmen of boor comms!\n",
      "Was ever man seeming with the mid-ain,\n",
      "Saves his beauty tenderle at thes?\n",
      "Halp heaven with her the main-courtier:\n",
      "His looks are rilgs of richmon! O that resist\n",
      "Are well foredue that I should not want,\n",
      "In that impartive,\n",
      "He was the tasses of heavenly here.\n",
      "\n",
      "LEONTES:\n",
      "His pranks she's not to do us,\n",
      "If wealty thousand name\n"
     ]
    }
   ],
   "source": [
    "# Generate the text with default temperature (1.0).\n",
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates text using the trained model with a given starting string \"ROMEO: \" and a temperature of 2.0. Setting a higher temperature leads to more diverse and unexpected results as the model is more likely to generate less probable next characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YsjAXbN3uxAw",
    "outputId": "d3ba6d75-09e0-4f4b-f06b-440e383716b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: thither is in safegrace,\n",
      "Your more stone with love to hear a traitor toum:\n",
      "His sakes, I tender-brain'd revenge,\n",
      "And lost alrains\n",
      "thou shouldst have due mispure,\n",
      "Were holder, flatter; and more razy, Quceemness an a belly?\n",
      "\n",
      "WARWICK:\n",
      "'scalistery\n",
      "With disease, consubel! sees your joy!\n",
      "\n",
      "Clown:\n",
      "Out upon him, visits,--a lay; for nt,\n",
      "His deady hath co-MISTER:\n",
      "At the o. fishize!'\n",
      "\n",
      "MENVIn\n",
      "Of Clarence get you LAND:\n",
      "Sands he:\n",
      "And tell my knife, thou hasst cr by the T:\n",
      "Once more, ovey it, draw\n",
      "Thy burrencembunckingham gaver:\n",
      "Even now ly powerful\n",
      "For our cabler suddens.\n",
      "\n",
      "DICHARD:\n",
      "Bring me have:\n",
      "It wanton Henry, pray, where's Gl as's huck we eater air\n",
      "For irst up against it: but I dather\n",
      "could fancing board repence,\n",
      "She had been broke.\n",
      "He usurpt living is to encounter.\n",
      "Ah, tidings,\n",
      "His ansare chifble.\n",
      "\n",
      "Third Monta:\n",
      "Come, my Katter with old command hate.\n",
      "Hark! Provostevend metal, BOLIO:\n",
      "\n",
      "Ritors dayly face, whom I, sort\n",
      "thou her marks: you will not Cholicly veriend\n",
      "Tybaltshion.\n",
      "Brother, Are-brace heard\n"
     ]
    }
   ],
   "source": [
    "# Generate the text with higher temperature to get more unexpected results.\n",
    "print(generate_text(model, start_string=u\"ROMEO: \", temperature=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfjd2ZjyvYf4"
   },
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code compiles the model with a loss function categorical_crossentropy, optimizer adam, and a single metric 'accuracy'. The compiled metrics can be viewed using the model.metrics property which returns a list of metric names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTsC8PRhvU9d",
    "outputId": "6631cb5f-daf3-4357-9a2a-d0fb17ab137d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with a loss function, optimizer, and metrics\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# View the list of compiled metrics\n",
    "print(model.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4m2q3Cru8Rn"
   },
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code saves the trained model in the HDF5 file format with the name text_generation_shakespeare_rnn.h5. The save() function from the Keras API is used to save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "NppDm0Pqu1js"
   },
   "outputs": [],
   "source": [
    "model_name = 'text_generation_shakespeare_rnn.h5'\n",
    "model.save(model_name, save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Future Exploration - Converting the model to web-format\n",
    "To use this model on the web we need to convert it into the format that will be understandable by tensorflowjs. To do so we may use tfjs-converter as following:\n",
    "\n",
    "tensorflowjs_converter --input_format keras \\\n",
    "  ./experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.h5 \\\n",
    "  ./demos/public/models/text_generation_shakespeare_rnn"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
